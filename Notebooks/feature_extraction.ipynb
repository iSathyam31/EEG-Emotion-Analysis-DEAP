{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import simpson\n",
    "from scipy.stats import entropy\n",
    "import pywt\n",
    "from joblib import Parallel, delayed  # For parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming all the channel in proper convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define the folder path\n",
    "folder_path = \"data_csv\"\n",
    "\n",
    "# Define new column names\n",
    "new_column_names = [\n",
    "    \"Fp1\", \"AF3\", \"F7\", \"F3\", \"FC1\", \"FC5\", \"T7\", \"C3\", \"CP1\", \"CP5\",\n",
    "    \"P7\", \"P3\", \"Pz\", \"PO3\", \"O1\", \"Oz\", \"O2\", \"PO4\", \"P4\", \"P8\",\n",
    "    \"CP6\", \"CP2\", \"C4\", \"T8\", \"FC6\", \"FC2\", \"F4\", \"F8\", \"AF4\", \"Fp2\",\n",
    "    \"Fz\", \"Cz\"\n",
    "]\n",
    "\n",
    "# Process each CSV file\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename first 32 columns\n",
    "        column_mapping = {f\"Channel_{i+1}\": new_column_names[i] for i in range(32)}\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "        # Save the modified CSV\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Column renaming completed for all CSV files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These channels can be divided into 5 regions of the brain*:\n",
    "\n",
    "| **Brain Region**  | **Electrode Channels**                                      |\n",
    "|------------------|------------------------------------------------------------|\n",
    "| **Frontal (F)**  | Fp1, Fp2, AF3, AF4, F7, F3, Fz, F4, F8                      |\n",
    "| **Central (C)**  | FC1, FC2, FC5, FC6, C3, Cz, C4                              |\n",
    "| **Temporal (T)** | T7, T8                                                      |\n",
    "| **Parietal (P)** | CP1, CP2, CP5, CP6, P3, P4, P7, P8, Pz                      |\n",
    "| **Occipital (O)**| O1, O2, Oz, PO3, PO4                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time-domain features\n",
    "def extract_time_features(signal):\n",
    "    features = {}\n",
    "    \n",
    "    # Statistical features\n",
    "    features['MEAN'] = np.mean(signal)\n",
    "    features['STD'] = np.std(signal)\n",
    "    features['MAX_VALUE'] = np.max(signal)\n",
    "    features['MIN_VALUE'] = np.min(signal)\n",
    "    features['SKEWNESS'] = skew(signal)\n",
    "    features['KURTOSIS'] = kurtosis(signal)\n",
    "    features['MEDIAN'] = np.median(signal)\n",
    "    \n",
    "    # First derivative features\n",
    "    first_diff = np.diff(signal)\n",
    "    features['1st_DIFF_MEAN'] = np.mean(first_diff)\n",
    "    features['1st_DIFF_MAX'] = np.max(first_diff)\n",
    "    \n",
    "    # Second derivative features\n",
    "    second_diff = np.diff(first_diff)\n",
    "    features['2nd_DIFF_MEAN'] = np.mean(second_diff)\n",
    "    features['2nd_DIFF_MAX'] = np.max(second_diff)\n",
    "    \n",
    "    # Hjorth parameters\n",
    "    diff_signal = np.diff(signal)\n",
    "    variance = np.var(signal)\n",
    "    activity = variance\n",
    "    mobility = np.sqrt(np.var(diff_signal) / variance)\n",
    "    complexity = np.sqrt(np.var(np.diff(diff_signal)) / np.var(diff_signal)) / mobility\n",
    "    features['HJORTH_ACTIVITY'] = activity\n",
    "    features['HJORTH_MOBILITY'] = mobility\n",
    "    features['HJORTH_COMPLEXITY'] = complexity\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to calculate frequency-domain features\n",
    "def extract_frequency_features(signal, fs=128):\n",
    "    features = {}\n",
    "\n",
    "    # Ensure signal is writable\n",
    "    signal = np.asarray(signal, dtype=np.float64, order='C').copy()\n",
    "\n",
    "    # FFT-based spectral features\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=1024)\n",
    "    delta_band = (0.1, 3)\n",
    "    theta_band = (3, 7)\n",
    "    alpha_band = (7, 12)\n",
    "    beta_band = (12, 30)\n",
    "    gamma_band = (30, 40)\n",
    "    whole_band = (0.1, 40)\n",
    "\n",
    "    def bandpower(band):\n",
    "        idx = np.logical_and(freqs >= band[0], freqs <= band[1])\n",
    "        return simpson(psd[idx], freqs[idx])  # Updated to simpson\n",
    "\n",
    "    features['FFT_DELTA'] = bandpower(delta_band)\n",
    "    features['FFT_THETA'] = bandpower(theta_band)\n",
    "    features['FFT_ALPHA'] = bandpower(alpha_band)\n",
    "    features['FFT_BETA'] = bandpower(beta_band)\n",
    "    features['FFT_GAMMA'] = bandpower(gamma_band)\n",
    "    features['FFT_WHOLE'] = bandpower(whole_band)\n",
    "\n",
    "    # Wavelet-based features\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=5)\n",
    "    wavelet_features = []\n",
    "    for coeff in coeffs:\n",
    "        wavelet_features.extend([\n",
    "            np.min(coeff), np.max(coeff), np.mean(coeff),\n",
    "            np.median(coeff), np.std(coeff), skew(coeff),\n",
    "            kurtosis(coeff), np.sum(coeff**2) / len(coeff)\n",
    "        ])\n",
    "    wavelet_columns = [\n",
    "        'MIN_WAV_VALUE', 'MAX_WAV_VALUE', 'MEAN_WAV_VALUE',\n",
    "        'MEDIAN_WAV_VALUE', 'STD_WAV_VALUE', 'SKEWNESS_WAV_VALUE',\n",
    "        'KURTOSIS_WAV_VALUE', 'WAV_BAND'\n",
    "    ]\n",
    "    for col, val in zip(wavelet_columns, wavelet_features):\n",
    "        features[col] = val\n",
    "\n",
    "    # Entropy-based features\n",
    "    features['ENTROPY_SPECTRAL'] = entropy(psd)\n",
    "    features['ENTROPY_SHANNON'] = entropy(np.histogram(signal, bins=20)[0])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Channel Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define channel groups for each brain region\n",
    "channel_groups = {\n",
    "    'FRONTAL': ['Fp1', 'Fp2', 'AF3', 'AF4', 'F7', 'F3', 'Fz', 'F4', 'F8'],\n",
    "    'CENTRAL': ['FC1', 'FC2', 'FC5', 'FC6', 'C3', 'Cz', 'C4'],\n",
    "    'TEMPORAL': ['T7', 'T8'],\n",
    "    'PARIETAL': ['CP1', 'CP2', 'CP5', 'CP6', 'P3', 'P4', 'P7', 'P8', 'Pz'],\n",
    "    'OCCIPITAL': ['O1', 'O2', 'Oz', 'PO3', 'PO4']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Compute Region-Averaged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_region_averages(trial_data):\n",
    "    region_features = {}\n",
    "    \n",
    "    for region, channels in channel_groups.items():\n",
    "        # Extract signals for all channels in the region\n",
    "        region_signals = [trial_data[ch].values for ch in channels]\n",
    "        \n",
    "        # Compute features for each channel in the region\n",
    "        channel_features = []\n",
    "        for signal in region_signals:\n",
    "            time_features = extract_time_features(signal)\n",
    "            freq_features = extract_frequency_features(signal)\n",
    "            channel_features.append({**time_features, **freq_features})\n",
    "        \n",
    "        # Average features across the region\n",
    "        for feature in channel_features[0].keys():\n",
    "            region_key = f'{region}_{feature}'\n",
    "            region_features[region_key] = np.mean([cf[feature] for cf in channel_features])\n",
    "    \n",
    "    return region_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Process a Single Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trial(trial_data, trial, subject_id):\n",
    "    \"\"\"\n",
    "    Processes a single trial and extracts region-averaged features.\n",
    "    \"\"\"\n",
    "    # Initialize feature dictionary for this trial\n",
    "    trial_features = {'Participant_ID': subject_id, 'Trial': trial}\n",
    "    \n",
    "    # Compute region-averaged features\n",
    "    region_features = compute_region_averages(trial_data)\n",
    "    trial_features.update(region_features)\n",
    "    \n",
    "    # Add emotion labels\n",
    "    trial_features['Valence'] = trial_data['Valence'].values[0]\n",
    "    trial_features['Arousal'] = trial_data['Arousal'].values[0]\n",
    "    trial_features['Dominance'] = trial_data['Dominance'].values[0]\n",
    "    trial_features['Liking'] = trial_data['Liking'].values[0]\n",
    "    \n",
    "    return trial_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function to extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_csv(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Extracts region-averaged features from all CSV files in the input folder and saves them to a single output file.\n",
    "    \"\"\"\n",
    "    all_trials = []  # List to store all trials' features\n",
    "    \n",
    "    # Loop through all CSV files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            subject_id = filename.split('.')[0]  # Extract subject ID (e.g., 's01')\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Process each trial in parallel\n",
    "            trials = Parallel(n_jobs=-1)(delayed(process_trial)(df[df['Trial'] == trial], trial, subject_id) \n",
    "                                        for trial in range(1, 41))\n",
    "            all_trials.extend(trials)\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df_features = pd.DataFrame(all_trials)\n",
    "    df_features.to_csv(output_file, index=False)\n",
    "    print(f\"Features extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted and saved to extracted_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_folder = \"data_csv\"  # Folder containing participant CSV files\n",
    "output_file = \"extracted_features.csv\"  # Path to save the extracted features\n",
    "\n",
    "# Run the feature extraction\n",
    "extract_features_from_csv(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelling the Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emotion(valence, arousal):\n",
    "    \"\"\"\n",
    "    Maps valence and arousal values to an emotion category.\n",
    "    \n",
    "    Parameters:\n",
    "    valence (float): Valence score (1 to 9).\n",
    "    arousal (float): Arousal score (1 to 9).\n",
    "    \n",
    "    Returns:\n",
    "    str: Emotion category.\n",
    "    \"\"\"\n",
    "    if valence > 5 and arousal > 5:\n",
    "        return \"Happy/Excited\"\n",
    "    elif valence > 5 and arousal <= 5:\n",
    "        return \"Relaxed/Content\"\n",
    "    elif valence <= 5 and arousal > 5:\n",
    "        return \"Angry/Stressed\"\n",
    "    elif valence <= 5 and arousal <= 5:\n",
    "        return \"Sad/Bored\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted features dataset\n",
    "df = pd.read_csv(\"extracted_features.csv\")\n",
    "\n",
    "# Apply the mapping function to create the Emotion column\n",
    "df['Emotion'] = df.apply(lambda row: map_emotion(row['Valence'], row['Arousal']), axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(\"extracted_features_with_emotion.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
